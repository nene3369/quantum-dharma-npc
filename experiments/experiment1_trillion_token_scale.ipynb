{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "id": "cell-0",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Data Selection - Experiment 1\n",
    "\n",
    "**兆トークン規模のストリーミング・スケッチベース量子データ選択**\n",
    "\n",
    "## 概要\n",
    "\n",
    "Experiment 0 では 100 サンプルの原理検証を行った。  \n",
    "本実験では、**1兆トークン規模**のデータセットに適用可能なアーキテクチャを設計・検証する。\n",
    "\n",
    "### 課題: なぜ Experiment 0 はスケールしないか\n",
    "\n",
    "| 問題 | Experiment 0 | 必要なスケール |\n",
    "|---|---|---|\n",
    "| QUBO サイズ | N=100 → 5,050 entries | N=10B docs → O(N²) は不可能 |\n",
    "| Surprise 計算 | 全データをメモリに保持 | 兆トークンはメモリに収まらない |\n",
    "| 多様性 | 未考慮 | 重複排除・カバレッジが必須 |\n",
    "| 処理時間 | 数分 | 数千GPU時間が現実的上限 |\n",
    "\n",
    "### 解決策: 3 層パイプライン\n",
    "\n",
    "```\n",
    "Raw Corpus (1T tokens)\n",
    "    │\n",
    "    ▼ Pass 1: Streaming Surprise + Sketch\n",
    "    │   - チャンク単位でストリーミング処理\n",
    "    │   - MinHash LSH で近似重複検出\n",
    "    │   - SimHash で多様性ベクトル生成\n",
    "    │   - 各ドキュメントに (surprise, minhash, simhash) タプル付与\n",
    "    │\n",
    "    ▼ Pass 2: Shard-Local QUBO\n",
    "    │   - データを S シャードに分割 (各 ~1M docs)\n",
    "    │   - 各シャード内で小規模 QUBO を量子アニーリングで解く\n",
    "    │   - シャード当たり K_local 個を選択\n",
    "    │\n",
    "    ▼ Pass 3: Global Merge QUBO\n",
    "        - 各シャードの選択結果 (S × K_local) を集約\n",
    "        - グローバル QUBO で最終 K_global 個を選択\n",
    "        - SimHash 多様性項でカバレッジを最大化\n",
    "```\n",
    "\n",
    "## 実行時間: 15-30分\n",
    "\n",
    "## 必要: D-Wave APIトークン"
   ]
  },
  {
   "id": "cell-1",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル1: インストール"
   ]
  },
  {
   "id": "cell-2",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install transformers datasets dwave-ocean-sdk torch matplotlib seaborn xxhash mmh3 -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-3",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル2: インポートと定数"
   ]
  },
  {
   "id": "cell-4",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import hashlib\n",
    "import struct\n",
    "from collections import defaultdict\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from dwave.system import LeapHybridSampler\n",
    "import dimod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Scaling constants (for theoretical analysis) ---\n",
    "TRILLION = 1_000_000_000_000\n",
    "AVG_TOKENS_PER_DOC = 500\n",
    "TOTAL_DOCS_1T = TRILLION // AVG_TOKENS_PER_DOC  # 2B documents\n",
    "\n",
    "# --- Demo constants (for actual execution) ---\n",
    "N_SAMPLES = 2000          # Simulate with 2K docs\n",
    "N_SHARDS = 4              # Number of shards\n",
    "K_LOCAL = 25              # Selections per shard\n",
    "K_GLOBAL = 20             # Final global selections\n",
    "MINHASH_PERMS = 128       # MinHash signature length\n",
    "SIMHASH_BITS = 64         # SimHash fingerprint bits\n",
    "LSH_BANDS = 16            # LSH band count\n",
    "LSH_ROWS = MINHASH_PERMS // LSH_BANDS  # Rows per band = 8\n",
    "\n",
    "print(\"All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"\\nTheoretical scale: {TOTAL_DOCS_1T:,} documents ({TRILLION/1e12:.0f}T tokens)\")\n",
    "print(f\"Demo scale: {N_SAMPLES:,} documents, {N_SHARDS} shards\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-5",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル3: D-Wave API 接続"
   ]
  },
  {
   "id": "cell-6",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# os.environ['DWAVE_API_TOKEN'] = 'your-token-here'\n",
    "\n",
    "try:\n",
    "    sampler = LeapHybridSampler()\n",
    "    print(\"D-Wave API connection successful\")\n",
    "    print(f\"  Solver: {sampler.solver.name}\")\n",
    "    USE_QUANTUM = True\n",
    "except Exception as e:\n",
    "    print(f\"D-Wave API connection failed: {e}\")\n",
    "    print(\"Falling back to simulated annealing (classical)\")\n",
    "    USE_QUANTUM = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-7",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル4: データ準備\n",
    "\n",
    "WikiText-103 から 2,000 サンプルをロード。  \n",
    "本番環境では The Pile / RedPajama / FineWeb 等の兆トークンコーパスを想定。"
   ]
  },
  {
   "id": "cell-8",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading WikiText-103 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "# 50文字以上のテキストのみ\n",
    "texts_raw = [x['text'] for x in dataset if len(x['text'].strip()) > 50]\n",
    "texts = texts_raw[:N_SAMPLES]\n",
    "\n",
    "print(f\"Loaded {len(texts)} text samples\")\n",
    "print(f\"Total characters: {sum(len(t) for t in texts):,}\")\n",
    "print(f\"Average length: {np.mean([len(t) for t in texts]):.0f} chars\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-9",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Streaming Surprise 計算\n",
    "\n",
    "### スケーリング戦略\n",
    "\n",
    "兆トークン規模では全データをメモリに保持できない。  \n",
    "ストリーミング処理で各ドキュメントを 1 パスで処理し、surprise を計算する。\n",
    "\n",
    "```\n",
    "ストリーム入力 → チャンク分割 → Proxy Model 推論 → surprise 値出力\n",
    "                    │                    │\n",
    "                    └── 固定メモリ ──────┘\n",
    "```\n",
    "\n",
    "**本番での最適化:**\n",
    "- Proxy Model: DistilGPT-2 (82M params) — 大規模モデルの 1/10 コスト\n",
    "- バッチ推論: GPU 上で 256-512 doc/batch\n",
    "- 推定スループット: A100 1台で ~50K docs/sec → 2B docs ≈ 11 時間"
   ]
  },
  {
   "id": "cell-10",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Loading DistilGPT-2 proxy model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-11",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_surprise_batch(texts_batch, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    バッチ単位で surprise を計算。\n",
    "    ストリーミング処理ではこの関数をチャンクごとに呼び出す。\n",
    "\n",
    "    Returns: list of float (surprise per document)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts_batch,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Per-token loss, then average per document\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        # outputs.loss is the mean over all tokens in the batch.\n",
    "        # For per-document loss, we need to compute manually.\n",
    "        logits = outputs.logits[:, :-1, :]  # (B, T-1, V)\n",
    "        labels = inputs[\"input_ids\"][:, 1:]  # (B, T-1)\n",
    "\n",
    "        # Create attention mask for non-pad tokens\n",
    "        attn = inputs[\"attention_mask\"][:, 1:]  # (B, T-1)\n",
    "\n",
    "        # Per-token cross-entropy\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        per_token_loss = loss_fn(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            labels.reshape(-1)\n",
    "        ).reshape(labels.shape)  # (B, T-1)\n",
    "\n",
    "        # Mask padding and average per document\n",
    "        masked_loss = per_token_loss * attn\n",
    "        doc_lengths = attn.sum(dim=1).clamp(min=1)\n",
    "        doc_surprises = (masked_loss.sum(dim=1) / doc_lengths).cpu().numpy()\n",
    "\n",
    "    return doc_surprises.tolist()\n",
    "\n",
    "\n",
    "def streaming_surprise(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    ストリーミング surprise 計算。\n",
    "    メモリ使用量は batch_size に比例し、データセットサイズに依存しない。\n",
    "    \"\"\"\n",
    "    all_surprises = []\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_surprises = compute_surprise_batch(batch, model, tokenizer, device)\n",
    "        all_surprises.extend(batch_surprises)\n",
    "\n",
    "        batch_idx = i // batch_size + 1\n",
    "        if batch_idx % 10 == 0 or batch_idx == n_batches:\n",
    "            elapsed = time.time() - t0\n",
    "            docs_per_sec = len(all_surprises) / elapsed\n",
    "            print(f\"  Batch {batch_idx}/{n_batches} | \"\n",
    "                  f\"{len(all_surprises)}/{len(texts)} docs | \"\n",
    "                  f\"{docs_per_sec:.0f} docs/sec\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    return np.array(all_surprises), elapsed\n",
    "\n",
    "\n",
    "print(\"Computing streaming surprises...\")\n",
    "surprises, surprise_time = streaming_surprise(texts, model, tokenizer, device)\n",
    "\n",
    "print(f\"\\nSurprise computation complete in {surprise_time:.1f}s\")\n",
    "print(f\"  Throughput: {len(texts) / surprise_time:.0f} docs/sec\")\n",
    "print(f\"  Mean surprise: {surprises.mean():.4f}\")\n",
    "print(f\"  Std surprise:  {surprises.std():.4f}\")\n",
    "\n",
    "# Scaling projection\n",
    "docs_per_sec = len(texts) / surprise_time\n",
    "gpu_speedup = 50  # Estimated A100 vs CPU speedup\n",
    "projected_a100_dps = docs_per_sec * gpu_speedup\n",
    "hours_for_2B = TOTAL_DOCS_1T / projected_a100_dps / 3600\n",
    "print(f\"\\n--- Scaling Projection ---\")\n",
    "print(f\"  Current throughput: {docs_per_sec:.0f} docs/sec\")\n",
    "print(f\"  Projected A100 throughput: {projected_a100_dps:,.0f} docs/sec\")\n",
    "print(f\"  Time for {TOTAL_DOCS_1T/1e9:.0f}B docs on 1x A100: {hours_for_2B:.0f} hours\")\n",
    "print(f\"  Time for {TOTAL_DOCS_1T/1e9:.0f}B docs on 64x A100: {hours_for_2B/64:.1f} hours\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-12",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: MinHash LSH (近似重複検出)\n",
    "\n",
    "### なぜ必要か\n",
    "\n",
    "兆トークンのウェブコーパスには大量の重複・近似重複が存在する。  \n",
    "同じ情報を持つドキュメントを複数選択することは無駄であり、  \n",
    "多様性を損なう。MinHash LSH で O(1) の近似重複検出を実現する。\n",
    "\n",
    "### アルゴリズム\n",
    "\n",
    "1. 各ドキュメントを n-gram (shingle) に分解\n",
    "2. 各 shingle をハッシュし、128 個のハッシュ関数の最小値を取る → MinHash 署名\n",
    "3. 署名を 16 バンドに分割し、各バンドのハッシュが一致するペアを近似重複候補とする\n",
    "4. Jaccard 類似度の閾値 (0.5) で重複判定\n",
    "\n",
    "**メモリ効率:** 署名は 128 × 4 bytes = 512 bytes/doc → 2B docs で ~1TB  \n",
    "**本番最適化:** Spark / MapReduce で分散処理、LSH インデックスはシャード分割"
   ]
  },
  {
   "id": "cell-13",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def text_to_shingles(text, k=5):\n",
    "    \"\"\"テキストを k-gram (shingle) セットに変換\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    if len(text) < k:\n",
    "        return set()\n",
    "    return set(text[i:i+k] for i in range(len(text) - k + 1))\n",
    "\n",
    "\n",
    "def minhash_signature(shingles, n_perms=MINHASH_PERMS, seed=42):\n",
    "    \"\"\"\n",
    "    MinHash 署名を計算。\n",
    "    n_perms 個の独立ハッシュ関数を使い、各ハッシュの最小値を署名とする。\n",
    "\n",
    "    本番では mmh3 や xxhash を使用して高速化。\n",
    "    ここでは再現性のため hashlib ベースで実装。\n",
    "    \"\"\"\n",
    "    if not shingles:\n",
    "        return np.zeros(n_perms, dtype=np.uint32)\n",
    "\n",
    "    signature = np.full(n_perms, np.iinfo(np.uint32).max, dtype=np.uint32)\n",
    "\n",
    "    for shingle in shingles:\n",
    "        shingle_bytes = shingle.encode('utf-8')\n",
    "        for i in range(n_perms):\n",
    "            # Hash(shingle || seed || perm_index)\n",
    "            h = hashlib.md5(shingle_bytes + struct.pack('<II', seed, i)).digest()\n",
    "            val = struct.unpack('<I', h[:4])[0]\n",
    "            if val < signature[i]:\n",
    "                signature[i] = val\n",
    "\n",
    "    return signature\n",
    "\n",
    "\n",
    "def lsh_buckets(signature, n_bands=LSH_BANDS):\n",
    "    \"\"\"\n",
    "    LSH バンディング: 署名を n_bands 個のバンドに分割し、\n",
    "    各バンドのハッシュをバケットキーとする。\n",
    "\n",
    "    同じバケットに入るペアは近似重複候補。\n",
    "    \"\"\"\n",
    "    rows_per_band = len(signature) // n_bands\n",
    "    buckets = []\n",
    "    for b in range(n_bands):\n",
    "        band = signature[b * rows_per_band : (b + 1) * rows_per_band]\n",
    "        bucket_key = hashlib.md5(band.tobytes()).hexdigest()\n",
    "        buckets.append((b, bucket_key))\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def estimated_jaccard(sig_a, sig_b):\n",
    "    \"\"\"MinHash 署名から Jaccard 類似度を推定\"\"\"\n",
    "    return np.mean(sig_a == sig_b)\n",
    "\n",
    "\n",
    "print(\"Computing MinHash signatures...\")\n",
    "t0 = time.time()\n",
    "\n",
    "signatures = []\n",
    "shingle_counts = []\n",
    "for i, text in enumerate(texts):\n",
    "    shingles = text_to_shingles(text, k=5)\n",
    "    sig = minhash_signature(shingles)\n",
    "    signatures.append(sig)\n",
    "    shingle_counts.append(len(shingles))\n",
    "\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  {i+1}/{N_SAMPLES} signatures computed\")\n",
    "\n",
    "minhash_time = time.time() - t0\n",
    "print(f\"\\nMinHash signatures computed in {minhash_time:.1f}s\")\n",
    "print(f\"  Throughput: {N_SAMPLES / minhash_time:.0f} docs/sec\")\n",
    "print(f\"  Avg shingles per doc: {np.mean(shingle_counts):.0f}\")\n",
    "print(f\"  Signature size: {MINHASH_PERMS * 4} bytes per doc\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-14",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Building LSH index and detecting near-duplicates...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Build LSH index: band -> bucket -> list of doc indices\n",
    "lsh_index = defaultdict(lambda: defaultdict(list))\n",
    "for doc_idx, sig in enumerate(signatures):\n",
    "    for band_id, bucket_key in lsh_buckets(sig):\n",
    "        lsh_index[band_id][bucket_key].append(doc_idx)\n",
    "\n",
    "# Find candidate pairs (docs sharing at least one bucket)\n",
    "candidate_pairs = set()\n",
    "for band_id in lsh_index:\n",
    "    for bucket_key, doc_indices in lsh_index[band_id].items():\n",
    "        if len(doc_indices) > 1:\n",
    "            for i in range(len(doc_indices)):\n",
    "                for j in range(i + 1, len(doc_indices)):\n",
    "                    pair = (min(doc_indices[i], doc_indices[j]),\n",
    "                            max(doc_indices[i], doc_indices[j]))\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "# Verify candidates with Jaccard threshold\n",
    "JACCARD_THRESHOLD = 0.5\n",
    "duplicate_pairs = []\n",
    "for i, j in candidate_pairs:\n",
    "    jaccard = estimated_jaccard(signatures[i], signatures[j])\n",
    "    if jaccard >= JACCARD_THRESHOLD:\n",
    "        duplicate_pairs.append((i, j, jaccard))\n",
    "\n",
    "# Build duplicate clusters (union-find)\n",
    "parent = list(range(N_SAMPLES))\n",
    "def find(x):\n",
    "    while parent[x] != x:\n",
    "        parent[x] = parent[parent[x]]\n",
    "        x = parent[x]\n",
    "    return x\n",
    "def union(a, b):\n",
    "    ra, rb = find(a), find(b)\n",
    "    if ra != rb:\n",
    "        parent[ra] = rb\n",
    "\n",
    "for i, j, _ in duplicate_pairs:\n",
    "    union(i, j)\n",
    "\n",
    "# Cluster analysis\n",
    "clusters = defaultdict(list)\n",
    "for i in range(N_SAMPLES):\n",
    "    clusters[find(i)].append(i)\n",
    "\n",
    "dup_clusters = {k: v for k, v in clusters.items() if len(v) > 1}\n",
    "\n",
    "# Duplicate mask: keep only cluster representative (highest surprise)\n",
    "is_duplicate = np.zeros(N_SAMPLES, dtype=bool)\n",
    "for cluster_id, members in dup_clusters.items():\n",
    "    # Keep the member with highest surprise\n",
    "    best = max(members, key=lambda idx: surprises[idx])\n",
    "    for m in members:\n",
    "        if m != best:\n",
    "            is_duplicate[m] = True\n",
    "\n",
    "lsh_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nLSH deduplication complete in {lsh_time:.1f}s\")\n",
    "print(f\"  Candidate pairs: {len(candidate_pairs):,}\")\n",
    "print(f\"  Confirmed duplicates: {len(duplicate_pairs):,}\")\n",
    "print(f\"  Duplicate clusters: {len(dup_clusters):,}\")\n",
    "print(f\"  Documents removed: {is_duplicate.sum()} ({is_duplicate.mean()*100:.1f}%)\")\n",
    "print(f\"  Remaining documents: {(~is_duplicate).sum()}\")\n",
    "\n",
    "if duplicate_pairs:\n",
    "    print(f\"\\n  Example duplicate pair:\")\n",
    "    i, j, jac = duplicate_pairs[0]\n",
    "    print(f\"    Doc {i}: '{texts[i][:80]}...'\")\n",
    "    print(f\"    Doc {j}: '{texts[j][:80]}...'\")\n",
    "    print(f\"    Jaccard similarity: {jac:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-15",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: SimHash (多様性フィンガープリント)\n",
    "\n",
    "### 目的\n",
    "\n",
    "QUBO の多様性項に使用する。2 つのドキュメント間の「内容的距離」を  \n",
    "SimHash の Hamming 距離で近似する。\n",
    "\n",
    "- Hamming 距離が大きい → 内容が異なる → 多様性が高い\n",
    "- Hamming 距離が小さい → 内容が類似 → 冗長\n",
    "\n",
    "### SimHash アルゴリズム\n",
    "\n",
    "1. テキストを特徴量（n-gram）に分解\n",
    "2. 各特徴量をハッシュし、64ビットベクトルに展開（0→-1, 1→+1）\n",
    "3. 全特徴量の加重和を取り、各ビットの符号で最終フィンガープリントを決定\n",
    "\n",
    "**メモリ効率:** 8 bytes/doc → 2B docs で ~16GB"
   ]
  },
  {
   "id": "cell-16",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def simhash(text, n_bits=SIMHASH_BITS, k=3):\n",
    "    \"\"\"\n",
    "    SimHash フィンガープリントを計算。\n",
    "\n",
    "    各 k-gram をハッシュし、ビットごとの加重和の符号を取る。\n",
    "    結果は n_bits ビットの整数。\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    if len(text) < k:\n",
    "        return 0\n",
    "\n",
    "    v = np.zeros(n_bits, dtype=np.float64)\n",
    "\n",
    "    for i in range(len(text) - k + 1):\n",
    "        gram = text[i:i+k]\n",
    "        h = int(hashlib.md5(gram.encode('utf-8')).hexdigest(), 16)\n",
    "        for bit in range(n_bits):\n",
    "            if (h >> bit) & 1:\n",
    "                v[bit] += 1.0\n",
    "            else:\n",
    "                v[bit] -= 1.0\n",
    "\n",
    "    # Convert to fingerprint\n",
    "    fingerprint = 0\n",
    "    for bit in range(n_bits):\n",
    "        if v[bit] > 0:\n",
    "            fingerprint |= (1 << bit)\n",
    "\n",
    "    return fingerprint\n",
    "\n",
    "\n",
    "def hamming_distance(a, b, n_bits=SIMHASH_BITS):\n",
    "    \"\"\"2つの SimHash 間の Hamming 距離\"\"\"\n",
    "    xor = a ^ b\n",
    "    return bin(xor).count('1')\n",
    "\n",
    "\n",
    "def hamming_to_diversity(dist, n_bits=SIMHASH_BITS):\n",
    "    \"\"\"Hamming 距離を [0, 1] の多様性スコアに変換\"\"\"\n",
    "    return dist / n_bits\n",
    "\n",
    "\n",
    "print(\"Computing SimHash fingerprints...\")\n",
    "t0 = time.time()\n",
    "\n",
    "simhashes = []\n",
    "for i, text in enumerate(texts):\n",
    "    sh = simhash(text)\n",
    "    simhashes.append(sh)\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  {i+1}/{N_SAMPLES} fingerprints computed\")\n",
    "\n",
    "simhash_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nSimHash computation complete in {simhash_time:.1f}s\")\n",
    "print(f\"  Throughput: {N_SAMPLES / simhash_time:.0f} docs/sec\")\n",
    "print(f\"  Fingerprint size: {SIMHASH_BITS // 8} bytes per doc\")\n",
    "\n",
    "# Sample pairwise distances\n",
    "sample_pairs = [(i, j) for i in range(min(100, N_SAMPLES))\n",
    "                       for j in range(i+1, min(100, N_SAMPLES))]\n",
    "sample_dists = [hamming_distance(simhashes[i], simhashes[j])\n",
    "                for i, j in sample_pairs[:500]]\n",
    "\n",
    "print(f\"\\n  Sample pairwise Hamming distances (first 100 docs):\")\n",
    "print(f\"    Mean: {np.mean(sample_dists):.1f} / {SIMHASH_BITS}\")\n",
    "print(f\"    Std:  {np.std(sample_dists):.1f}\")\n",
    "print(f\"    Min:  {np.min(sample_dists)}\")\n",
    "print(f\"    Max:  {np.max(sample_dists)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-17",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 階層的 QUBO (Shard-Local → Global Merge)\n",
    "\n",
    "### スケーリング問題と解決策\n",
    "\n",
    "N=2B ドキュメントで直接 QUBO を構築すると O(N²) = O(4 × 10¹⁸) エントリ → 不可能。\n",
    "\n",
    "**階層的アプローチ:**\n",
    "\n",
    "```\n",
    "2B docs ÷ 2000 shards = 1M docs/shard\n",
    "  │\n",
    "  ├── Shard 1: QUBO(1M) → select 500 (top-K pre-filter + QUBO)\n",
    "  ├── Shard 2: QUBO(1M) → select 500\n",
    "  ├── ...\n",
    "  └── Shard 2000: QUBO(1M) → select 500\n",
    "         │\n",
    "         ▼\n",
    "  Global: QUBO(1M candidates) → select 100K final\n",
    "```\n",
    "\n",
    "**各シャードの QUBO (拡張版):**\n",
    "\n",
    "$$H_{\\text{shard}} = -\\alpha \\sum_i S_i x_i + \\beta \\sum_{i<j} \\text{dup}(i,j) x_i x_j - \\delta \\sum_{i<j} d_H(i,j) x_i x_j + \\gamma \\left(\\sum_i x_i - K\\right)^2$$\n",
    "\n",
    "| 項 | 意味 | 効果 |\n",
    "|---|---|---|\n",
    "| $-\\alpha S_i$ | Surprise 最大化 | 高情報価値を選択 |\n",
    "| $\\beta \\cdot \\text{dup}$ | 重複ペナルティ | MinHash で検出した重複を排除 |\n",
    "| $-\\delta \\cdot d_H$ | 多様性ボーナス | SimHash 距離が大きいペアを優遇 |\n",
    "| $\\gamma (\\sum - K)^2$ | カーディナリティ制約 | ちょうど K 個選択 |"
   ]
  },
  {
   "id": "cell-18",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_enhanced_qubo(surprises, signatures, simhashes, is_duplicate,\n",
    "                        doc_indices, K,\n",
    "                        alpha=1.0, beta=5.0, delta=0.3, gamma=10.0):\n",
    "    \"\"\"\n",
    "    拡張 QUBO 行列を構築。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    surprises : array - 各ドキュメントの surprise 値\n",
    "    signatures : list - MinHash 署名\n",
    "    simhashes : list - SimHash フィンガープリント\n",
    "    is_duplicate : array - 重複フラグ\n",
    "    doc_indices : list - このシャード内のドキュメントインデックス\n",
    "    K : int - 選択数\n",
    "    alpha : float - Surprise の重み\n",
    "    beta : float - 重複ペナルティの重み\n",
    "    delta : float - 多様性ボーナスの重み\n",
    "    gamma : float - カーディナリティ制約の重み\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : dict - QUBO 行列\n",
    "    var_to_doc : dict - QUBO 変数 → ドキュメントインデックスのマッピング\n",
    "    \"\"\"\n",
    "    # Filter out already-removed duplicates\n",
    "    valid_docs = [idx for idx in doc_indices if not is_duplicate[idx]]\n",
    "    N = len(valid_docs)\n",
    "    var_to_doc = {var: doc for var, doc in enumerate(valid_docs)}\n",
    "\n",
    "    Q = {}\n",
    "\n",
    "    # Normalize surprises for this shard\n",
    "    shard_surprises = np.array([surprises[var_to_doc[v]] for v in range(N)])\n",
    "    if shard_surprises.std() > 0:\n",
    "        norm_surprises = (shard_surprises - shard_surprises.mean()) / shard_surprises.std()\n",
    "    else:\n",
    "        norm_surprises = np.zeros(N)\n",
    "\n",
    "    # Diagonal terms: -alpha * S_i + gamma * (1 - 2K)\n",
    "    for v in range(N):\n",
    "        Q[(v, v)] = -alpha * norm_surprises[v] + gamma * (1 - 2 * K)\n",
    "\n",
    "    # Off-diagonal terms\n",
    "    for vi in range(N):\n",
    "        for vj in range(vi + 1, N):\n",
    "            doc_i = var_to_doc[vi]\n",
    "            doc_j = var_to_doc[vj]\n",
    "\n",
    "            # Cardinality constraint\n",
    "            val = 2 * gamma\n",
    "\n",
    "            # Duplicate penalty (MinHash Jaccard > threshold)\n",
    "            jaccard = estimated_jaccard(signatures[doc_i], signatures[doc_j])\n",
    "            if jaccard > 0.3:  # Soft penalty starts at 0.3\n",
    "                val += beta * jaccard\n",
    "\n",
    "            # Diversity bonus (SimHash Hamming distance)\n",
    "            h_dist = hamming_to_diversity(\n",
    "                hamming_distance(simhashes[doc_i], simhashes[doc_j]))\n",
    "            val -= delta * h_dist\n",
    "\n",
    "            Q[(vi, vj)] = val\n",
    "\n",
    "    return Q, var_to_doc\n",
    "\n",
    "\n",
    "def solve_qubo(Q, use_quantum=True, label='shard'):\n",
    "    \"\"\"\n",
    "    QUBO を解く。量子アニーリングが利用可能なら使用、なければ SA。\n",
    "    \"\"\"\n",
    "    if use_quantum and USE_QUANTUM:\n",
    "        sampler = LeapHybridSampler()\n",
    "        response = sampler.sample_qubo(Q, label=label)\n",
    "    else:\n",
    "        bqm = dimod.BinaryQuadraticModel.from_qubo(Q)\n",
    "        sampler = dimod.SimulatedAnnealingSampler()\n",
    "        response = sampler.sample(bqm, num_reads=100, num_sweeps=1000)\n",
    "\n",
    "    solution = response.first.sample\n",
    "    energy = response.first.energy\n",
    "    selected = [v for v, val in solution.items() if val == 1]\n",
    "    return selected, energy\n",
    "\n",
    "\n",
    "print(\"Enhanced QUBO builder ready\")\n",
    "print(f\"  QUBO terms: surprise (alpha), dedup (beta), diversity (delta), constraint (gamma)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-19",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル12: シャード分割と Shard-Local QUBO 実行"
   ]
  },
  {
   "id": "cell-20",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Splitting {N_SAMPLES} documents into {N_SHARDS} shards...\")\n",
    "\n",
    "# Assign documents to shards (round-robin; in production, use hash-based)\n",
    "shard_assignments = [[] for _ in range(N_SHARDS)]\n",
    "for i in range(N_SAMPLES):\n",
    "    shard_assignments[i % N_SHARDS].append(i)\n",
    "\n",
    "for s in range(N_SHARDS):\n",
    "    n_valid = sum(1 for idx in shard_assignments[s] if not is_duplicate[idx])\n",
    "    print(f\"  Shard {s}: {len(shard_assignments[s])} docs ({n_valid} after dedup)\")\n",
    "\n",
    "print(f\"\\nRunning shard-local QUBO selection (K_local={K_LOCAL} per shard)...\")\n",
    "print(f\"  Use quantum: {USE_QUANTUM}\")\n",
    "print()\n",
    "\n",
    "shard_results = []\n",
    "total_qubo_time = 0\n",
    "\n",
    "for s in range(N_SHARDS):\n",
    "    t0 = time.time()\n",
    "\n",
    "    Q, var_to_doc = build_enhanced_qubo(\n",
    "        surprises, signatures, simhashes, is_duplicate,\n",
    "        doc_indices=shard_assignments[s],\n",
    "        K=K_LOCAL,\n",
    "        alpha=1.0, beta=5.0, delta=0.3, gamma=10.0\n",
    "    )\n",
    "\n",
    "    selected_vars, energy = solve_qubo(\n",
    "        Q,\n",
    "        use_quantum=USE_QUANTUM,\n",
    "        label=f'QDS-Exp1-Shard{s}'\n",
    "    )\n",
    "\n",
    "    # Map QUBO variables back to document indices\n",
    "    selected_docs = [var_to_doc[v] for v in selected_vars if v in var_to_doc]\n",
    "    shard_time = time.time() - t0\n",
    "    total_qubo_time += shard_time\n",
    "\n",
    "    avg_surprise = surprises[selected_docs].mean() if selected_docs else 0\n",
    "\n",
    "    print(f\"  Shard {s}: selected {len(selected_docs)}/{K_LOCAL} docs | \"\n",
    "          f\"energy={energy:.1f} | avg_surprise={avg_surprise:.4f} | \"\n",
    "          f\"time={shard_time:.1f}s\")\n",
    "\n",
    "    shard_results.append({\n",
    "        'shard': s,\n",
    "        'selected_docs': selected_docs,\n",
    "        'energy': energy,\n",
    "        'avg_surprise': avg_surprise\n",
    "    })\n",
    "\n",
    "# Merge all shard selections\n",
    "all_shard_selected = []\n",
    "for r in shard_results:\n",
    "    all_shard_selected.extend(r['selected_docs'])\n",
    "\n",
    "print(f\"\\nShard-local selection complete\")\n",
    "print(f\"  Total selected: {len(all_shard_selected)} docs\")\n",
    "print(f\"  Total QUBO time: {total_qubo_time:.1f}s\")\n",
    "print(f\"  Avg surprise (all shards): {surprises[all_shard_selected].mean():.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-21",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セル13: Global Merge QUBO\n",
    "\n",
    "各シャードの選択結果を集約し、最終的な K_global 個を選択する。  \n",
    "この段階ではシャード間の多様性が特に重要。"
   ]
  },
  {
   "id": "cell-22",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Running global merge QUBO...\")\n",
    "print(f\"  Input: {len(all_shard_selected)} candidates from {N_SHARDS} shards\")\n",
    "print(f\"  Target: K_global = {K_GLOBAL}\")\n",
    "\n",
    "# For global merge, we want stronger diversity emphasis\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a dummy is_duplicate array (no additional dedup at global level)\n",
    "global_no_dup = np.zeros(N_SAMPLES, dtype=bool)\n",
    "\n",
    "Q_global, var_to_doc_global = build_enhanced_qubo(\n",
    "    surprises, signatures, simhashes, global_no_dup,\n",
    "    doc_indices=all_shard_selected,\n",
    "    K=K_GLOBAL,\n",
    "    alpha=1.0, beta=3.0, delta=0.5, gamma=12.0  # Higher diversity weight at global level\n",
    ")\n",
    "\n",
    "global_selected_vars, global_energy = solve_qubo(\n",
    "    Q_global,\n",
    "    use_quantum=USE_QUANTUM,\n",
    "    label='QDS-Exp1-GlobalMerge'\n",
    ")\n",
    "\n",
    "# Map back to document indices\n",
    "global_selected = [var_to_doc_global[v] for v in global_selected_vars\n",
    "                   if v in var_to_doc_global]\n",
    "\n",
    "global_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nGlobal merge complete in {global_time:.1f}s\")\n",
    "print(f\"  Selected: {len(global_selected)} docs\")\n",
    "print(f\"  Energy: {global_energy:.1f}\")\n",
    "print(f\"  Avg surprise: {surprises[global_selected].mean():.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-23",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: ベースライン比較\n",
    "\n",
    "3 つの選択手法を比較する:\n",
    "\n",
    "1. **Quantum Hierarchical** (本手法): Surprise + MinHash + SimHash + 階層 QUBO\n",
    "2. **Top-K Surprise** (貪欲法): Surprise 上位 K 個を選択（多様性なし）\n",
    "3. **Random**: ランダムに K 個を選択"
   ]
  },
  {
   "id": "cell-24",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BASELINE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Baseline 1: Top-K Surprise (greedy) ---\n",
    "# Remove duplicates first, then take top K by surprise\n",
    "non_dup_indices = [i for i in range(N_SAMPLES) if not is_duplicate[i]]\n",
    "sorted_by_surprise = sorted(non_dup_indices, key=lambda i: surprises[i], reverse=True)\n",
    "topk_selected = sorted_by_surprise[:K_GLOBAL]\n",
    "\n",
    "# --- Baseline 2: Random ---\n",
    "N_RANDOM_TRIALS = 200\n",
    "random_results = []\n",
    "for _ in range(N_RANDOM_TRIALS):\n",
    "    random_sel = np.random.choice(non_dup_indices, K_GLOBAL, replace=False)\n",
    "    random_results.append({\n",
    "        'avg_surprise': surprises[random_sel].mean(),\n",
    "        'indices': random_sel\n",
    "    })\n",
    "random_avg_surprise = np.mean([r['avg_surprise'] for r in random_results])\n",
    "random_std_surprise = np.std([r['avg_surprise'] for r in random_results])\n",
    "\n",
    "# --- Compute diversity for each method ---\n",
    "def compute_diversity(selected_indices):\n",
    "    \"\"\"選択されたドキュメント間の平均 SimHash 多様性\"\"\"\n",
    "    if len(selected_indices) < 2:\n",
    "        return 0.0\n",
    "    total_dist = 0\n",
    "    n_pairs = 0\n",
    "    for i in range(len(selected_indices)):\n",
    "        for j in range(i + 1, len(selected_indices)):\n",
    "            total_dist += hamming_to_diversity(\n",
    "                hamming_distance(\n",
    "                    simhashes[selected_indices[i]],\n",
    "                    simhashes[selected_indices[j]]))\n",
    "            n_pairs += 1\n",
    "    return total_dist / n_pairs if n_pairs > 0 else 0.0\n",
    "\n",
    "quantum_diversity = compute_diversity(global_selected)\n",
    "topk_diversity = compute_diversity(topk_selected)\n",
    "\n",
    "# Random diversity (average over trials)\n",
    "random_diversities = []\n",
    "for r in random_results[:20]:  # Sample 20 trials for speed\n",
    "    random_diversities.append(compute_diversity(r['indices'].tolist()))\n",
    "random_avg_diversity = np.mean(random_diversities)\n",
    "\n",
    "# --- Results table ---\n",
    "quantum_surprise = surprises[global_selected].mean()\n",
    "topk_surprise = surprises[topk_selected].mean()\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Avg Surprise':>15} {'Diversity':>12} {'Count':>8}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Quantum Hierarchical':<30} {quantum_surprise:>15.4f} {quantum_diversity:>12.4f} {len(global_selected):>8}\")\n",
    "print(f\"{'Top-K Surprise (greedy)':<30} {topk_surprise:>15.4f} {topk_diversity:>12.4f} {len(topk_selected):>8}\")\n",
    "print(f\"{'Random (n=200 trials)':<30} {random_avg_surprise:>15.4f} {random_avg_diversity:>12.4f} {K_GLOBAL:>8}\")\n",
    "print(f\"{'  Random std':<30} {'+/-' + f'{random_std_surprise:.4f}':>15}\")\n",
    "\n",
    "# --- Composite score: balances surprise and diversity ---\n",
    "# Normalize to [0,1] range based on observed values\n",
    "all_surp = [quantum_surprise, topk_surprise, random_avg_surprise]\n",
    "surp_min, surp_max = min(all_surp), max(all_surp)\n",
    "surp_range = surp_max - surp_min if surp_max > surp_min else 1.0\n",
    "\n",
    "all_div = [quantum_diversity, topk_diversity, random_avg_diversity]\n",
    "div_min, div_max = min(all_div), max(all_div)\n",
    "div_range = div_max - div_min if div_max > div_min else 1.0\n",
    "\n",
    "def composite_score(surprise, diversity, w_surp=0.6, w_div=0.4):\n",
    "    norm_s = (surprise - surp_min) / surp_range\n",
    "    norm_d = (diversity - div_min) / div_range\n",
    "    return w_surp * norm_s + w_div * norm_d\n",
    "\n",
    "print(f\"\\n{'Composite Score (0.6×Surprise + 0.4×Diversity)':}\")\n",
    "print(f\"  Quantum:  {composite_score(quantum_surprise, quantum_diversity):.4f}\")\n",
    "print(f\"  Top-K:    {composite_score(topk_surprise, topk_diversity):.4f}\")\n",
    "print(f\"  Random:   {composite_score(random_avg_surprise, random_avg_diversity):.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-25",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 可視化"
   ]
  },
  {
   "id": "cell-26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# --- Plot 1: Surprise distribution with selections ---\n",
    "ax = axes[0, 0]\n",
    "ax.hist(surprises, bins=50, alpha=0.5, color='gray', label='All docs')\n",
    "ax.hist(surprises[global_selected], bins=20, alpha=0.7, color='red',\n",
    "        label=f'Quantum ({len(global_selected)})')\n",
    "ax.hist(surprises[topk_selected], bins=20, alpha=0.5, color='green',\n",
    "        label=f'Top-K ({len(topk_selected)})')\n",
    "ax.set_xlabel('Surprise')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Surprise Distribution')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Per-shard selection quality ---\n",
    "ax = axes[0, 1]\n",
    "shard_labels = [f'Shard {r[\"shard\"]}' for r in shard_results]\n",
    "shard_avg_surprises = [r['avg_surprise'] for r in shard_results]\n",
    "shard_counts = [len(r['selected_docs']) for r in shard_results]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, N_SHARDS))\n",
    "bars = ax.bar(shard_labels, shard_avg_surprises, color=colors)\n",
    "for bar, count in zip(bars, shard_counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "ax.axhline(quantum_surprise, color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'Global avg: {quantum_surprise:.3f}')\n",
    "ax.set_ylabel('Avg Surprise')\n",
    "ax.set_title('Shard-Local Selection Quality')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 3: Diversity comparison ---\n",
    "ax = axes[0, 2]\n",
    "methods = ['Quantum\\nHierarchical', 'Top-K\\nSurprise', 'Random']\n",
    "diversities = [quantum_diversity, topk_diversity, random_avg_diversity]\n",
    "bar_colors = ['red', 'green', 'blue']\n",
    "ax.bar(methods, diversities, color=bar_colors, alpha=0.7)\n",
    "ax.set_ylabel('Diversity (avg pairwise SimHash distance)')\n",
    "ax.set_title('Selection Diversity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 4: Surprise vs Diversity scatter ---\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(quantum_surprise, quantum_diversity, c='red', s=200, marker='*',\n",
    "           zorder=5, label='Quantum')\n",
    "ax.scatter(topk_surprise, topk_diversity, c='green', s=200, marker='s',\n",
    "           zorder=5, label='Top-K')\n",
    "# Random trials as a cloud\n",
    "for r, d in zip(random_results[:50], random_diversities[:50] if len(random_diversities) >= 50\n",
    "                else random_diversities):\n",
    "    ax.scatter(r['avg_surprise'], d, c='blue', s=20, alpha=0.3)\n",
    "ax.scatter(random_avg_surprise, random_avg_diversity, c='blue', s=200, marker='o',\n",
    "           zorder=5, label='Random (avg)')\n",
    "ax.set_xlabel('Average Surprise')\n",
    "ax.set_ylabel('Diversity')\n",
    "ax.set_title('Surprise-Diversity Pareto')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 5: Document selection map ---\n",
    "ax = axes[1, 1]\n",
    "quantum_set = set(global_selected)\n",
    "topk_set = set(topk_selected)\n",
    "colors_map = []\n",
    "for i in range(N_SAMPLES):\n",
    "    if i in quantum_set and i in topk_set:\n",
    "        colors_map.append('purple')\n",
    "    elif i in quantum_set:\n",
    "        colors_map.append('red')\n",
    "    elif i in topk_set:\n",
    "        colors_map.append('green')\n",
    "    elif is_duplicate[i]:\n",
    "        colors_map.append('orange')\n",
    "    else:\n",
    "        colors_map.append('lightgray')\n",
    "ax.scatter(range(N_SAMPLES), surprises, c=colors_map, s=15, alpha=0.6)\n",
    "ax.set_xlabel('Document index')\n",
    "ax.set_ylabel('Surprise')\n",
    "ax.set_title('Selection Map (red=quantum, green=top-k, purple=both, orange=dup)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 6: Random baseline distribution ---\n",
    "ax = axes[1, 2]\n",
    "random_surp_list = [r['avg_surprise'] for r in random_results]\n",
    "ax.hist(random_surp_list, bins=30, alpha=0.7, color='blue', label='Random trials')\n",
    "ax.axvline(quantum_surprise, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Quantum: {quantum_surprise:.4f}')\n",
    "ax.axvline(topk_surprise, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Top-K: {topk_surprise:.4f}')\n",
    "ax.set_xlabel('Avg Surprise')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Random Baseline Distribution')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment1_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Visualization saved: experiment1_results.png\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-27",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: 兆トークン規模のスケーリング分析"
   ]
  },
  {
   "id": "cell-28",
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 70)\nprint(\"SCALING ANALYSIS: Experiment 0 → 1 → Production\")\nprint(\"=\" * 70)\n\n# Measured timings from this experiment\nmeasured = {\n    'n_docs': N_SAMPLES,\n    'surprise_time': surprise_time,\n    'minhash_time': minhash_time,\n    'simhash_time': simhash_time,\n    'lsh_time': lsh_time,\n    'qubo_time': total_qubo_time + global_time,\n}\n\nprint(f\"\\n--- Measured Timings (N={N_SAMPLES:,} docs) ---\")\nprint(f\"  Surprise computation:  {measured['surprise_time']:>8.1f}s\")\nprint(f\"  MinHash signatures:    {measured['minhash_time']:>8.1f}s\")\nprint(f\"  SimHash fingerprints:  {measured['simhash_time']:>8.1f}s\")\nprint(f\"  LSH deduplication:     {measured['lsh_time']:>8.1f}s\")\nprint(f\"  QUBO solving:          {measured['qubo_time']:>8.1f}s\")\ntotal_measured = sum(measured[k] for k in ['surprise_time', 'minhash_time',\n                                            'simhash_time', 'lsh_time', 'qubo_time'])\nprint(f\"  Total:                 {total_measured:>8.1f}s\")\n\n# Scaling projections\nscales = [\n    ('Experiment 0', 100, 1),\n    ('Experiment 1 (demo)', N_SAMPLES, 1),\n    ('Medium scale', 1_000_000, 1),\n    ('Large scale', 100_000_000, 64),\n    ('Production (1T tokens)', TOTAL_DOCS_1T, 256),\n]\n\nprint(f\"\\n--- Scaling Projections ---\")\nprint(f\"{'Scale':<25} {'N docs':>15} {'GPUs':>6} {'Pass 1':>12} {'Pass 2':>12} {'Pass 3':>12} {'Total':>12}\")\nprint(\"-\" * 100)\n\nfor name, n_docs, n_gpus in scales:\n    scale_factor = n_docs / N_SAMPLES\n\n    # Pass 1: Streaming (linear, parallelizable)\n    pass1_sec = (measured['surprise_time'] + measured['minhash_time'] +\n                 measured['simhash_time']) * scale_factor / n_gpus\n\n    # Pass 2: Shard-local QUBO\n    # Each shard is ~1M docs max, number of shards grows linearly\n    n_shards_projected = max(1, n_docs // 1_000_000)\n    # QUBO per shard is O(K_local^2) which is constant; LSH is ~O(N/shard)\n    pass2_sec = (measured['qubo_time'] / N_SHARDS) * n_shards_projected / max(1, n_gpus // 4)\n    pass2_sec += measured['lsh_time'] * scale_factor / n_gpus\n\n    # Pass 3: Global merge QUBO (fixed size: S * K_local candidates)\n    # QUBO size = O((S * K_local)^2)\n    global_candidates = min(n_shards_projected * K_LOCAL, 100_000)\n    pass3_sec = global_time * (global_candidates / len(all_shard_selected)) ** 2\n    pass3_sec = min(pass3_sec, 300)  # D-Wave hybrid solver caps at ~5min\n\n    total_sec = pass1_sec + pass2_sec + pass3_sec\n\n    def fmt_time(s):\n        if s < 60:\n            return f\"{s:.0f}s\"\n        elif s < 3600:\n            return f\"{s/60:.0f}min\"\n        elif s < 86400:\n            return f\"{s/3600:.1f}hr\"\n        else:\n            return f\"{s/86400:.1f}day\"\n\n    print(f\"{name:<25} {n_docs:>15,} {n_gpus:>6} {fmt_time(pass1_sec):>12} \"\n          f\"{fmt_time(pass2_sec):>12} {fmt_time(pass3_sec):>12} {fmt_time(total_sec):>12}\")\n\nprint(f\"\\n--- Memory Requirements ---\")\nprint(f\"{'Component':<25} {'Per Doc':>10} {'2B docs':>15}\")\nprint(\"-\" * 55)\nmem_items = [\n    ('Surprise (float32)', 4, TOTAL_DOCS_1T * 4),\n    ('MinHash sig (128*u32)', 512, TOTAL_DOCS_1T * 512),\n    ('SimHash (u64)', 8, TOTAL_DOCS_1T * 8),\n    ('Duplicate flag (bool)', 1, TOTAL_DOCS_1T),\n    ('Shard metadata', 16, TOTAL_DOCS_1T * 16),\n]\ntotal_mem = 0\nfor comp_name, per_doc, total_bytes in mem_items:\n    total_mem += total_bytes\n    if total_bytes > 1e12:\n        print(f\"{comp_name:<25} {per_doc:>10} B {total_bytes/1e12:>12.1f} TB\")\n    else:\n        print(f\"{comp_name:<25} {per_doc:>10} B {total_bytes/1e9:>12.1f} GB\")\nprint(\"-\" * 55)\nprint(f\"{'Total':<25} {'':>10} {total_mem/1e12:>12.1f} TB\")\nprint(f\"\\n  Note: MinHash signatures dominate. Options to reduce:\")\nprint(f\"    - Use 64 perms instead of 128 -> {TOTAL_DOCS_1T * 256 / 1e12:.1f} TB\")\nprint(f\"    - Use uint16 hashes -> {TOTAL_DOCS_1T * 256 / 1e12:.1f} TB\")\nprint(f\"    - Shard-local LSH (no global MinHash storage needed)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-29",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: D-Wave QPU 使用量分析"
   ]
  },
  {
   "id": "cell-30",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"D-WAVE QPU USAGE PROJECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# D-Wave Leap pricing (approximate)\n",
    "# Hybrid solver: ~$0.10 per minute of solver time\n",
    "# Free tier: 20 minutes/month\n",
    "\n",
    "print(f\"\\n--- Current Experiment ---\")\n",
    "n_qubo_calls = N_SHARDS + 1  # shards + global\n",
    "avg_call_time = (total_qubo_time + global_time) / n_qubo_calls\n",
    "print(f\"  QUBO calls: {n_qubo_calls}\")\n",
    "print(f\"  Avg call time: {avg_call_time:.1f}s\")\n",
    "print(f\"  Total solver time: {total_qubo_time + global_time:.1f}s\")\n",
    "\n",
    "print(f\"\\n--- Production Projection (2B docs) ---\")\n",
    "prod_shards = TOTAL_DOCS_1T // 1_000_000  # 2000 shards\n",
    "prod_qubo_calls = prod_shards + 1\n",
    "prod_solver_minutes = prod_qubo_calls * avg_call_time / 60\n",
    "prod_cost = prod_solver_minutes * 0.10\n",
    "\n",
    "print(f\"  Shards: {prod_shards:,}\")\n",
    "print(f\"  QUBO calls: {prod_qubo_calls:,}\")\n",
    "print(f\"  Estimated solver time: {prod_solver_minutes:,.0f} minutes ({prod_solver_minutes/60:.0f} hours)\")\n",
    "print(f\"  Estimated cost: ${prod_cost:,.0f}\")\n",
    "\n",
    "print(f\"\\n--- Optimization Strategies ---\")\n",
    "print(f\"  1. Pre-filter top-50% by surprise before QUBO (halve problem size)\")\n",
    "print(f\"  2. Use SA for low-value shards, quantum only for competitive shards\")\n",
    "print(f\"  3. Hierarchical merge: local→regional→global (3 levels instead of 2)\")\n",
    "print(f\"  4. Cache QUBO solutions for similar surprise distributions\")\n",
    "\n",
    "# Optimized projection\n",
    "opt_shards = prod_shards\n",
    "pct_quantum = 0.1  # Only 10% of shards use quantum\n",
    "opt_quantum_calls = int(opt_shards * pct_quantum) + 10  # 10 regional + global\n",
    "opt_solver_min = opt_quantum_calls * avg_call_time / 60\n",
    "opt_cost = opt_solver_min * 0.10\n",
    "print(f\"\\n  Optimized (10% quantum shards):\")\n",
    "print(f\"    Quantum calls: {opt_quantum_calls:,}\")\n",
    "print(f\"    Solver time: {opt_solver_min:,.0f} minutes ({opt_solver_min/60:.0f} hours)\")\n",
    "print(f\"    Cost: ${opt_cost:,.0f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "cell-31",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: まとめと次のステップ"
   ]
  },
  {
   "id": "cell-32",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT 1 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Key Results:\n",
    "  1. Streaming surprise computation: {len(texts)/surprise_time:.0f} docs/sec\n",
    "     → Projects to {TOTAL_DOCS_1T/1e9:.0f}B docs in ~{TOTAL_DOCS_1T / (len(texts)/surprise_time * 50) / 3600 / 64:.0f} hours on 64x A100\n",
    "\n",
    "  2. MinHash LSH deduplication: {is_duplicate.sum()} duplicates removed ({is_duplicate.mean()*100:.1f}%)\n",
    "     → O(1) per-document lookup, shard-parallelizable\n",
    "\n",
    "  3. SimHash diversity: avg pairwise distance {quantum_diversity:.3f}\n",
    "     → 8 bytes/doc fingerprint, enables QUBO diversity term\n",
    "\n",
    "  4. Hierarchical QUBO: {N_SHARDS} shards → global merge\n",
    "     → Quantum surprise: {quantum_surprise:.4f}\n",
    "     → Top-K surprise:   {topk_surprise:.4f}\n",
    "     → Random surprise:  {random_avg_surprise:.4f}\n",
    "\n",
    "Architecture Validated:\n",
    "  - 3-pass pipeline (Stream → Shard QUBO → Global Merge)\n",
    "  - Each pass is independently parallelizable\n",
    "  - QUBO size bounded by shard size, not total corpus\n",
    "  - D-Wave hybrid solver handles shard-local optimization\n",
    "\n",
    "Next Steps (Experiment 2):\n",
    "  1. Downstream validation: train small LM on quantum-selected vs random data\n",
    "  2. Measure perplexity/benchmark improvement\n",
    "  3. Test with larger corpus (WikiText-103 full or C4 subset)\n",
    "  4. Optimize QUBO parameters (alpha, beta, delta, gamma) via grid search\n",
    "  5. Compare with D4RL / DSIR / other data selection baselines\n",
    "\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}