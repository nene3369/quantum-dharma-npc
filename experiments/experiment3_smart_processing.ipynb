{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "id": "cell-0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantum Data Selection - Experiment 3\n",
        "\n",
        "**賢く選んで、賢く処理する**\n",
        "\n",
        "## 問題\n",
        "\n",
        "Experiment 0-2 では「賢く選ぶ」ことに注力した。  \n",
        "しかし選択後の学習はバニラ fine-tuning — surprise スコアも多様性情報も捨てている。\n",
        "\n",
        "```\n",
        "Exp 0-2: [賢く選ぶ] ─── vanilla train ──→ model\n",
        "                         ↑ ここが雑\n",
        "\n",
        "Exp 3:   [賢く選ぶ] ─── [賢く処理する] ──→ model\n",
        "```\n",
        "\n",
        "## 3つの「賢い処理」戦略\n",
        "\n",
        "### Strategy A: Curriculum Learning (カリキュラム学習)\n",
        "\n",
        "人間の学習と同じ — 簡単なものから難しいものへ順に学ぶ。  \n",
        "Surprise スコアで難易度順序をつけ、学習順序を制御する。\n",
        "\n",
        "```\n",
        "Epoch 1: 低 Surprise (簡単、パターン学習)\n",
        "Epoch 2: 中 Surprise (応用、構造理解)\n",
        "Epoch 3: 高 Surprise (難問、エッジケース)\n",
        "```\n",
        "\n",
        "### Strategy B: Surprise-Weighted Loss (情報価値加重損失)\n",
        "\n",
        "全てのデータを等しく扱うのではなく、  \n",
        "高 Surprise = 高情報価値のデータにより大きな勾配を割り当てる。\n",
        "\n",
        "$$\\mathcal{L}_{\\text{weighted}} = \\frac{1}{N} \\sum_i w(S_i) \\cdot \\ell_i$$\n",
        "\n",
        "ここで $w(S_i) = \\text{softmax}(S_i / \\tau)$ で温度 $\\tau$ が情報集中度を制御。\n",
        "\n",
        "### Strategy C: Active Iteration (能動的反復選択)\n",
        "\n",
        "一度選んで一度学習するのではなく、学習→再評価→再選択のループ。  \n",
        "モデルが更新されると surprise 分布が変わるため、  \n",
        "「今のモデルにとって」最も情報価値の高いデータを動的に選択する。\n",
        "\n",
        "```\n",
        "Round 1: Select 200 docs → Train 1 epoch → Re-score pool\n",
        "Round 2: Select 200 docs → Train 1 epoch → Re-score pool  \n",
        "Round 3: Select 100 docs → Train 1 epoch → Done\n",
        "```\n",
        "\n",
        "## 実験設計\n",
        "\n",
        "| 手法 | 選択 | 処理 |\n",
        "|---|---|---|\n",
        "| Baseline (Exp2) | Quantum QUBO | Vanilla FT |\n",
        "| Strategy A | Quantum QUBO | Curriculum (easy→hard) |\n",
        "| Strategy B | Quantum QUBO | Surprise-weighted loss |\n",
        "| Strategy C | Active QUBO | Active iteration (3 rounds) |\n",
        "| Full Pipeline | Active QUBO | Curriculum + Weighted |\n",
        "\n",
        "## 実行時間: 40-60分 (GPU推奨)\n",
        "## 必要: D-Wave APIトークン, GPU"
      ]
    },
    {
      "id": "cell-1",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## セル1: セットアップ"
      ]
    },
    {
      "id": "cell-2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install transformers datasets dwave-ocean-sdk torch matplotlib seaborn scipy -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import hashlib\n",
        "import struct\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from scipy import stats\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from dwave.system import LeapHybridSampler\n",
        "import dimod\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Config ---\n",
        "N_POOL = 5000\n",
        "K_SELECT = 500\n",
        "N_SHARDS = 5\n",
        "K_LOCAL = 50\n",
        "MINHASH_PERMS = 128\n",
        "SIMHASH_BITS = 64\n",
        "LSH_BANDS = 16\n",
        "\n",
        "TRAIN_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 128\n",
        "WARMUP_STEPS = 50\n",
        "\n",
        "# Active iteration config\n",
        "ACTIVE_ROUNDS = 3\n",
        "ACTIVE_K_PER_ROUND = [200, 200, 100]  # Total = 500\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "# os.environ['DWAVE_API_TOKEN'] = 'your-token-here'\n",
        "\n",
        "try:\n",
        "    sampler = LeapHybridSampler()\n",
        "    USE_QUANTUM = True\n",
        "    print(\"D-Wave connected\")\n",
        "except Exception as e:\n",
        "    USE_QUANTUM = False\n",
        "    print(f\"D-Wave unavailable, using SA: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-5",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## セル3: データ準備 + Surprise 計算 + スケッチ\n",
        "\n",
        "Experiment 2 と同じパイプラインを再利用。"
      ]
    },
    {
      "id": "cell-6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Data ---\n",
        "print(\"Loading data...\")\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "train_texts_all = [x['text'] for x in dataset['train'] if len(x['text'].strip()) > 80]\n",
        "np.random.seed(42)\n",
        "pool_indices = np.random.choice(len(train_texts_all), N_POOL, replace=False)\n",
        "pool_texts = [train_texts_all[i] for i in pool_indices]\n",
        "test_texts = [x['text'] for x in dataset['validation'] if len(x['text'].strip()) > 80][:500]\n",
        "print(f\"Pool: {len(pool_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "# --- Proxy model ---\n",
        "print(\"Loading proxy model...\")\n",
        "proxy_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device).eval()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def compute_surprises(texts, model, batch_size=32):\n",
        "    \"\"\"Compute per-document surprise with a given model\"\"\"\n",
        "    all_s = []\n",
        "    model.eval()\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,\n",
        "                           max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits[:, :-1, :]\n",
        "            labels = inputs[\"input_ids\"][:, 1:]\n",
        "            attn = inputs[\"attention_mask\"][:, 1:]\n",
        "            loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "            pt = loss_fn(logits.reshape(-1, logits.size(-1)),\n",
        "                         labels.reshape(-1)).reshape(labels.shape)\n",
        "            lengths = attn.sum(dim=1).clamp(min=1)\n",
        "            all_s.extend(((pt * attn).sum(dim=1) / lengths).cpu().numpy().tolist())\n",
        "    return np.array(all_s)\n",
        "\n",
        "print(\"Computing initial surprises...\")\n",
        "surprises = compute_surprises(pool_texts, proxy_model)\n",
        "print(f\"Surprise: mean={surprises.mean():.4f}, std={surprises.std():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Sketch functions (MinHash + SimHash + LSH) ---\n",
        "\n",
        "def text_to_shingles(text, k=5):\n",
        "    text = text.lower().strip()\n",
        "    return set(text[i:i+k] for i in range(len(text)-k+1)) if len(text) >= k else set()\n",
        "\n",
        "def minhash_signature(shingles, n_perms=MINHASH_PERMS, seed=42):\n",
        "    if not shingles:\n",
        "        return np.zeros(n_perms, dtype=np.uint32)\n",
        "    sig = np.full(n_perms, np.iinfo(np.uint32).max, dtype=np.uint32)\n",
        "    for sh in shingles:\n",
        "        sb = sh.encode('utf-8')\n",
        "        for i in range(n_perms):\n",
        "            v = struct.unpack('<I', hashlib.md5(sb + struct.pack('<II', seed, i)).digest()[:4])[0]\n",
        "            if v < sig[i]: sig[i] = v\n",
        "    return sig\n",
        "\n",
        "def estimated_jaccard(a, b): return np.mean(a == b)\n",
        "\n",
        "def simhash_fp(text, n_bits=SIMHASH_BITS, k=3):\n",
        "    text = text.lower().strip()\n",
        "    if len(text) < k: return 0\n",
        "    v = np.zeros(n_bits)\n",
        "    for i in range(len(text)-k+1):\n",
        "        h = int(hashlib.md5(text[i:i+k].encode()).hexdigest(), 16)\n",
        "        for b in range(n_bits): v[b] += 1.0 if (h >> b) & 1 else -1.0\n",
        "    fp = 0\n",
        "    for b in range(n_bits):\n",
        "        if v[b] > 0: fp |= (1 << b)\n",
        "    return fp\n",
        "\n",
        "def hamming_dist(a, b): return bin(a ^ b).count('1')\n",
        "def hamming_div(a, b): return hamming_dist(a, b) / SIMHASH_BITS\n",
        "\n",
        "# Compute all sketches\n",
        "print(\"Computing sketches...\")\n",
        "signatures = []\n",
        "simhashes = []\n",
        "for i, t in enumerate(pool_texts):\n",
        "    signatures.append(minhash_signature(text_to_shingles(t)))\n",
        "    simhashes.append(simhash_fp(t))\n",
        "    if (i+1) % 1000 == 0: print(f\"  {i+1}/{N_POOL}\")\n",
        "\n",
        "# LSH dedup\n",
        "lsh = defaultdict(lambda: defaultdict(list))\n",
        "for idx, sig in enumerate(signatures):\n",
        "    rows = MINHASH_PERMS // LSH_BANDS\n",
        "    for b in range(LSH_BANDS):\n",
        "        band = sig[b*rows:(b+1)*rows]\n",
        "        lsh[b][hashlib.md5(band.tobytes()).hexdigest()].append(idx)\n",
        "\n",
        "parent = list(range(N_POOL))\n",
        "def find(x):\n",
        "    while parent[x] != x: parent[x] = parent[parent[x]]; x = parent[x]\n",
        "    return x\n",
        "def union(a, b):\n",
        "    a, b = find(a), find(b)\n",
        "    if a != b: parent[a] = b\n",
        "\n",
        "for bid in lsh:\n",
        "    for key, docs in lsh[bid].items():\n",
        "        if len(docs) > 1:\n",
        "            for a in range(len(docs)):\n",
        "                for b in range(a+1, len(docs)):\n",
        "                    if estimated_jaccard(signatures[docs[a]], signatures[docs[b]]) >= 0.5:\n",
        "                        union(docs[a], docs[b])\n",
        "\n",
        "clusters = defaultdict(list)\n",
        "for i in range(N_POOL): clusters[find(i)].append(i)\n",
        "is_duplicate = np.zeros(N_POOL, dtype=bool)\n",
        "for _, members in clusters.items():\n",
        "    if len(members) > 1:\n",
        "        best = max(members, key=lambda i: surprises[i])\n",
        "        for m in members:\n",
        "            if m != best: is_duplicate[m] = True\n",
        "\n",
        "print(f\"Duplicates removed: {is_duplicate.sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-8",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## セル5: QUBO ソルバー (共通)"
      ]
    },
    {
      "id": "cell-9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_qubo(surprises, signatures, simhashes, is_dup, doc_indices, K,\n",
        "               alpha=1.0, beta=5.0, delta=0.3, gamma=10.0):\n",
        "    valid = [i for i in doc_indices if not is_dup[i]]\n",
        "    N = len(valid)\n",
        "    v2d = {v: d for v, d in enumerate(valid)}\n",
        "    Q = {}\n",
        "    s_arr = np.array([surprises[v2d[v]] for v in range(N)])\n",
        "    s_norm = (s_arr - s_arr.mean()) / s_arr.std() if s_arr.std() > 0 else np.zeros(N)\n",
        "    for v in range(N):\n",
        "        Q[(v, v)] = -alpha * s_norm[v] + gamma * (1 - 2*K)\n",
        "    for vi in range(N):\n",
        "        for vj in range(vi+1, N):\n",
        "            val = 2 * gamma\n",
        "            jac = estimated_jaccard(signatures[v2d[vi]], signatures[v2d[vj]])\n",
        "            if jac > 0.3: val += beta * jac\n",
        "            val -= delta * hamming_div(simhashes[v2d[vi]], simhashes[v2d[vj]])\n",
        "            Q[(vi, vj)] = val\n",
        "    return Q, v2d\n",
        "\n",
        "def solve_qubo(Q, label='q'):\n",
        "    if USE_QUANTUM:\n",
        "        resp = LeapHybridSampler().sample_qubo(Q, label=label)\n",
        "    else:\n",
        "        bqm = dimod.BinaryQuadraticModel.from_qubo(Q)\n",
        "        resp = dimod.SimulatedAnnealingSampler().sample(bqm, num_reads=200, num_sweeps=2000)\n",
        "    sol = resp.first.sample\n",
        "    return [v for v, x in sol.items() if x == 1], resp.first.energy\n",
        "\n",
        "def hierarchical_select(surprises, signatures, simhashes, is_dup, K,\n",
        "                        n_shards=N_SHARDS, k_local=K_LOCAL, label_prefix='Q'):\n",
        "    \"\"\"Full hierarchical QUBO selection pipeline\"\"\"\n",
        "    shards = [[] for _ in range(n_shards)]\n",
        "    for i in range(N_POOL): shards[i % n_shards].append(i)\n",
        "\n",
        "    all_selected = []\n",
        "    for s in range(n_shards):\n",
        "        Q, v2d = build_qubo(surprises, signatures, simhashes, is_dup,\n",
        "                            shards[s], k_local)\n",
        "        sel, _ = solve_qubo(Q, label=f'{label_prefix}-S{s}')\n",
        "        all_selected.extend([v2d[v] for v in sel if v in v2d])\n",
        "\n",
        "    # Global merge\n",
        "    no_dup = np.zeros(N_POOL, dtype=bool)\n",
        "    Q_g, v2d_g = build_qubo(surprises, signatures, simhashes, no_dup,\n",
        "                            all_selected, K, alpha=1.0, beta=3.0, delta=0.5, gamma=12.0)\n",
        "    g_sel, _ = solve_qubo(Q_g, label=f'{label_prefix}-Global')\n",
        "    return [v2d_g[v] for v in g_sel if v in v2d_g]\n",
        "\n",
        "print(\"QUBO solver ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-10",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## セル6: 共通評価関数"
      ]
    },
    {
      "id": "cell-11",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=MAX_LENGTH):\n",
        "        self.enc = tokenizer(texts, truncation=True, max_length=max_length,\n",
        "                             padding=\"max_length\", return_tensors=\"pt\")\n",
        "    def __len__(self): return self.enc[\"input_ids\"].shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.enc[\"input_ids\"][idx],\n",
        "                \"attention_mask\": self.enc[\"attention_mask\"][idx]}\n",
        "\n",
        "\n",
        "def evaluate_ppl(model, test_texts, tokenizer):\n",
        "    \"\"\"Compute perplexity on test set\"\"\"\n",
        "    model.eval()\n",
        "    ds = TextDataset(test_texts, tokenizer)\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE)\n",
        "    total_loss, total_tokens = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            logits = model(input_ids=ids, attention_mask=mask).logits[:, :-1, :]\n",
        "            labels = ids[:, 1:]\n",
        "            m = mask[:, 1:]\n",
        "            pt = nn.CrossEntropyLoss(reduction='none')(\n",
        "                logits.reshape(-1, logits.size(-1)), labels.reshape(-1)\n",
        "            ).reshape(labels.shape)\n",
        "            total_loss += (pt * m).sum().item()\n",
        "            total_tokens += m.sum().item()\n",
        "    return np.exp(total_loss / total_tokens)\n",
        "\n",
        "\n",
        "# Base PPL\n",
        "print(\"Evaluating base model...\")\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device).eval()\n",
        "base_ppl = evaluate_ppl(base_model, test_texts, tokenizer)\n",
        "print(f\"Base PPL: {base_ppl:.2f}\")\n",
        "del base_model; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-12",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: Static Quantum Selection (Exp2 再現 = ベースライン)"
      ]
    },
    {
      "id": "cell-13",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Static Quantum Selection (Exp2 baseline)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "static_selected = hierarchical_select(\n",
        "    surprises, signatures, simhashes, is_duplicate,\n",
        "    K=K_SELECT, label_prefix='Exp3-Static'\n",
        ")\n",
        "print(f\"Selected {len(static_selected)} docs\")\n",
        "print(f\"Avg surprise: {surprises[static_selected].mean():.4f}\")\n",
        "\n",
        "# Store surprise scores for selected docs (used by strategies)\n",
        "selected_surprises = {idx: surprises[idx] for idx in static_selected}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-14",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Training Strategies\n",
        "\n",
        "### Strategy 0: Vanilla Fine-tune (control)"
      ]
    },
    {
      "id": "cell-15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_vanilla(train_texts, test_texts, tokenizer, run_name, epochs=TRAIN_EPOCHS):\n",
        "    \"\"\"Standard fine-tuning (Exp2 reproduction)\"\"\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "    ds = TextDataset(train_texts, tokenizer)\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    sched = get_linear_schedule_with_warmup(opt, WARMUP_STEPS, len(loader) * epochs)\n",
        "\n",
        "    results = {'train_losses': [], 'eval_ppls': []}\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total, n = 0, 0\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            loss = model(input_ids=ids, attention_mask=mask, labels=ids).loss\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sched.step(); opt.zero_grad()\n",
        "            total += loss.item(); n += 1\n",
        "        ppl = evaluate_ppl(model, test_texts, tokenizer)\n",
        "        results['train_losses'].append(total / n)\n",
        "        results['eval_ppls'].append(ppl)\n",
        "        print(f\"  [{run_name}] Ep {ep+1}: loss={total/n:.4f} ppl={ppl:.2f}\")\n",
        "\n",
        "    results['final_ppl'] = results['eval_ppls'][-1]\n",
        "    del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    return results\n",
        "\n",
        "print(\"\\nTraining: Vanilla (control)\")\n",
        "vanilla_texts = [pool_texts[i] for i in static_selected]\n",
        "results_vanilla = train_vanilla(vanilla_texts, test_texts, tokenizer, 'Vanilla')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-16",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy A: Curriculum Learning\n",
        "\n",
        "Surprise スコアで 3 段階に分け、各 epoch で異なる難易度帯を学習する。  \n",
        "「易→難」の順序が学習効率を高める (Bengio et al., 2009)。"
      ]
    },
    {
      "id": "cell-17",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_curriculum(train_indices, pool_texts, surprises, test_texts,\n",
        "                     tokenizer, run_name, epochs=TRAIN_EPOCHS):\n",
        "    \"\"\"\n",
        "    Curriculum learning: sort by surprise, train easy→hard.\n",
        "\n",
        "    Epoch 1: bottom 1/3 (easy)\n",
        "    Epoch 2: middle 1/3 (medium)\n",
        "    Epoch 3: top 1/3 (hard)\n",
        "    \"\"\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "    # Sort by surprise (ascending = easy first)\n",
        "    sorted_indices = sorted(train_indices, key=lambda i: surprises[i])\n",
        "    n = len(sorted_indices)\n",
        "    thirds = [sorted_indices[:n//3], sorted_indices[n//3:2*n//3], sorted_indices[2*n//3:]]\n",
        "    difficulty_names = ['easy', 'medium', 'hard']\n",
        "\n",
        "    results = {'train_losses': [], 'eval_ppls': []}\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        # Progressive curriculum: include all previous + current tier\n",
        "        curriculum_indices = []\n",
        "        for t in range(ep + 1):\n",
        "            if t < len(thirds):\n",
        "                curriculum_indices.extend(thirds[t])\n",
        "\n",
        "        tier_name = '+'.join(difficulty_names[:ep+1])\n",
        "        curr_texts = [pool_texts[i] for i in curriculum_indices]\n",
        "        ds = TextDataset(curr_texts, tokenizer)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "        # Adjust scheduler per epoch\n",
        "        sched = get_linear_schedule_with_warmup(opt, 10, len(loader))\n",
        "\n",
        "        model.train()\n",
        "        total, nb = 0, 0\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            loss = model(input_ids=ids, attention_mask=mask, labels=ids).loss\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sched.step(); opt.zero_grad()\n",
        "            total += loss.item(); nb += 1\n",
        "\n",
        "        ppl = evaluate_ppl(model, test_texts, tokenizer)\n",
        "        results['train_losses'].append(total / nb)\n",
        "        results['eval_ppls'].append(ppl)\n",
        "        print(f\"  [{run_name}] Ep {ep+1} ({tier_name}, {len(curriculum_indices)} docs): \"\n",
        "              f\"loss={total/nb:.4f} ppl={ppl:.2f}\")\n",
        "\n",
        "    results['final_ppl'] = results['eval_ppls'][-1]\n",
        "    del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    return results\n",
        "\n",
        "print(\"\\nTraining: Curriculum (easy -> hard)\")\n",
        "results_curriculum = train_curriculum(\n",
        "    static_selected, pool_texts, surprises, test_texts, tokenizer, 'Curriculum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-18",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy B: Surprise-Weighted Loss\n",
        "\n",
        "各サンプルの勾配を surprise スコアで重み付けする。  \n",
        "高 surprise = 高情報量 → より大きな学習シグナル。\n",
        "\n",
        "$$w_i = \\text{softmax}(S_i / \\tau)$$\n",
        "\n",
        "温度 $\\tau$ が低いほど高 surprise に集中、高いほど均一に近づく。"
      ]
    },
    {
      "id": "cell-19",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class WeightedTextDataset(Dataset):\n",
        "    \"\"\"Dataset with per-sample surprise weights\"\"\"\n",
        "    def __init__(self, texts, weights, tokenizer, max_length=MAX_LENGTH):\n",
        "        self.enc = tokenizer(texts, truncation=True, max_length=max_length,\n",
        "                             padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self): return self.enc[\"input_ids\"].shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.enc[\"input_ids\"][idx],\n",
        "                \"attention_mask\": self.enc[\"attention_mask\"][idx],\n",
        "                \"weight\": self.weights[idx]}\n",
        "\n",
        "\n",
        "def train_weighted(train_indices, pool_texts, surprises, test_texts,\n",
        "                   tokenizer, run_name, tau=1.0, epochs=TRAIN_EPOCHS):\n",
        "    \"\"\"\n",
        "    Surprise-weighted loss training.\n",
        "\n",
        "    Each sample's loss is scaled by softmax(surprise / tau).\n",
        "    \"\"\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "\n",
        "    # Compute softmax weights from surprise scores\n",
        "    s_scores = np.array([surprises[i] for i in train_indices])\n",
        "    exp_s = np.exp((s_scores - s_scores.max()) / tau)  # numerically stable\n",
        "    weights = exp_s / exp_s.sum() * len(train_indices)  # scale so mean(w) = 1\n",
        "\n",
        "    train_texts = [pool_texts[i] for i in train_indices]\n",
        "    ds = WeightedTextDataset(train_texts, weights, tokenizer)\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    sched = get_linear_schedule_with_warmup(opt, WARMUP_STEPS, len(loader) * epochs)\n",
        "\n",
        "    results = {'train_losses': [], 'eval_ppls': []}\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total, nb = 0, 0\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            w = batch[\"weight\"].to(device)  # (B,)\n",
        "\n",
        "            logits = model(input_ids=ids, attention_mask=mask).logits[:, :-1, :]\n",
        "            labels = ids[:, 1:]\n",
        "            m = mask[:, 1:]\n",
        "\n",
        "            # Per-token loss\n",
        "            loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "            pt_loss = loss_fn(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                labels.reshape(-1)\n",
        "            ).reshape(labels.shape)  # (B, T)\n",
        "\n",
        "            # Per-document weighted average\n",
        "            doc_loss = (pt_loss * m).sum(dim=1) / m.sum(dim=1).clamp(min=1)  # (B,)\n",
        "            weighted_loss = (doc_loss * w).mean()  # scalar\n",
        "\n",
        "            weighted_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sched.step(); opt.zero_grad()\n",
        "            total += weighted_loss.item(); nb += 1\n",
        "\n",
        "        ppl = evaluate_ppl(model, test_texts, tokenizer)\n",
        "        results['train_losses'].append(total / nb)\n",
        "        results['eval_ppls'].append(ppl)\n",
        "        print(f\"  [{run_name}] Ep {ep+1}: loss={total/nb:.4f} ppl={ppl:.2f}\")\n",
        "\n",
        "    results['final_ppl'] = results['eval_ppls'][-1]\n",
        "    del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\\nTraining: Surprise-Weighted Loss (tau=1.0)\")\n",
        "results_weighted = train_weighted(\n",
        "    static_selected, pool_texts, surprises, test_texts,\n",
        "    tokenizer, 'Weighted-1.0', tau=1.0)\n",
        "\n",
        "print(\"\\nTraining: Surprise-Weighted Loss (tau=0.5, more concentrated)\")\n",
        "results_weighted_hot = train_weighted(\n",
        "    static_selected, pool_texts, surprises, test_texts,\n",
        "    tokenizer, 'Weighted-0.5', tau=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-20",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy C: Active Iteration\n",
        "\n",
        "最も強力な戦略。学習中のモデルで surprise を再計算し、  \n",
        "「今のモデルにとって最も情報価値の高いデータ」を動的に選択する。\n",
        "\n",
        "```\n",
        "Round 1: proxy model → surprise → QUBO → select 200 → train 1 epoch\n",
        "Round 2: updated model → surprise → QUBO → select 200 → train 1 epoch\n",
        "Round 3: updated model → surprise → QUBO → select 100 → train 1 epoch\n",
        "```"
      ]
    },
    {
      "id": "cell-21",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_active(pool_texts, surprises_init, signatures, simhashes, is_dup,\n",
        "                 test_texts, tokenizer, run_name,\n",
        "                 rounds=ACTIVE_ROUNDS, k_per_round=ACTIVE_K_PER_ROUND):\n",
        "    \"\"\"\n",
        "    Active iteration: select → train → re-score → select → ...\n",
        "\n",
        "    Each round:\n",
        "    1. Compute surprise with CURRENT model\n",
        "    2. Run QUBO on remaining pool\n",
        "    3. Train on newly selected data for 1 epoch\n",
        "    \"\"\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "    all_selected = set()\n",
        "    results = {'train_losses': [], 'eval_ppls': [], 'round_selections': [],\n",
        "               'surprise_shifts': []}\n",
        "\n",
        "    # Track how surprises change across rounds\n",
        "    current_surprises = surprises_init.copy()\n",
        "\n",
        "    for rd in range(rounds):\n",
        "        k = k_per_round[rd] if rd < len(k_per_round) else k_per_round[-1]\n",
        "\n",
        "        # 1. Re-compute surprise with current model (except round 0 = proxy)\n",
        "        if rd > 0:\n",
        "            print(f\"  Round {rd+1}: Re-scoring pool with updated model...\")\n",
        "            current_surprises = compute_surprises(pool_texts, model)\n",
        "            shift = np.abs(current_surprises - surprises_init).mean()\n",
        "            results['surprise_shifts'].append(float(shift))\n",
        "            print(f\"    Mean surprise shift: {shift:.4f}\")\n",
        "        else:\n",
        "            results['surprise_shifts'].append(0.0)\n",
        "\n",
        "        # 2. Exclude already-selected docs\n",
        "        remaining = [i for i in range(N_POOL)\n",
        "                     if i not in all_selected and not is_dup[i]]\n",
        "\n",
        "        if len(remaining) < k:\n",
        "            print(f\"  Round {rd+1}: Only {len(remaining)} docs remaining, selecting all\")\n",
        "            new_selected = remaining\n",
        "        else:\n",
        "            # 3. QUBO on remaining pool with updated surprises\n",
        "            # Use simplified single-shard for remaining pool\n",
        "            Q, v2d = build_qubo(\n",
        "                current_surprises, signatures, simhashes, is_dup,\n",
        "                remaining[:min(len(remaining), 1000)],  # Cap for speed\n",
        "                K=k, alpha=1.0, beta=5.0, delta=0.3, gamma=10.0\n",
        "            )\n",
        "            sel_vars, _ = solve_qubo(Q, label=f'{run_name}-R{rd}')\n",
        "            new_selected = [v2d[v] for v in sel_vars if v in v2d]\n",
        "\n",
        "        all_selected.update(new_selected)\n",
        "        results['round_selections'].append(len(new_selected))\n",
        "\n",
        "        # 4. Train on ALL selected so far for 1 epoch\n",
        "        all_train_texts = [pool_texts[i] for i in all_selected]\n",
        "        ds = TextDataset(all_train_texts, tokenizer)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        sched = get_linear_schedule_with_warmup(opt, 10, len(loader))\n",
        "\n",
        "        model.train()\n",
        "        total, nb = 0, 0\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            loss = model(input_ids=ids, attention_mask=mask, labels=ids).loss\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sched.step(); opt.zero_grad()\n",
        "            total += loss.item(); nb += 1\n",
        "\n",
        "        ppl = evaluate_ppl(model, test_texts, tokenizer)\n",
        "        results['train_losses'].append(total / nb)\n",
        "        results['eval_ppls'].append(ppl)\n",
        "\n",
        "        print(f\"  [{run_name}] Round {rd+1}: +{len(new_selected)} docs \"\n",
        "              f\"(total {len(all_selected)}), loss={total/nb:.4f}, ppl={ppl:.2f}\")\n",
        "\n",
        "    results['final_ppl'] = results['eval_ppls'][-1]\n",
        "    results['total_selected'] = len(all_selected)\n",
        "    results['selected_indices'] = list(all_selected)\n",
        "    del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\\nTraining: Active Iteration (3 rounds)\")\n",
        "results_active = train_active(\n",
        "    pool_texts, surprises, signatures, simhashes, is_duplicate,\n",
        "    test_texts, tokenizer, 'Active')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-22",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy D: Full Pipeline (Active + Curriculum + Weighted)\n",
        "\n",
        "3 つの戦略を統合した最強パイプライン:\n",
        "\n",
        "1. Active Iteration で動的にデータを選択\n",
        "2. 各ラウンド内で Curriculum 順序 (easy→hard)\n",
        "3. Surprise-Weighted Loss で勾配を調整"
      ]
    },
    {
      "id": "cell-23",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_full_pipeline(pool_texts, surprises_init, signatures, simhashes, is_dup,\n",
        "                        test_texts, tokenizer, run_name, tau=1.0,\n",
        "                        rounds=ACTIVE_ROUNDS, k_per_round=ACTIVE_K_PER_ROUND):\n",
        "    \"\"\"\n",
        "    Full pipeline: Active selection + Curriculum order + Weighted loss.\n",
        "    \"\"\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "    all_selected = set()\n",
        "    results = {'train_losses': [], 'eval_ppls': [], 'round_selections': [],\n",
        "               'surprise_shifts': []}\n",
        "    current_surprises = surprises_init.copy()\n",
        "\n",
        "    for rd in range(rounds):\n",
        "        k = k_per_round[rd] if rd < len(k_per_round) else k_per_round[-1]\n",
        "\n",
        "        # Active: re-score with current model\n",
        "        if rd > 0:\n",
        "            current_surprises = compute_surprises(pool_texts, model)\n",
        "            shift = np.abs(current_surprises - surprises_init).mean()\n",
        "            results['surprise_shifts'].append(float(shift))\n",
        "        else:\n",
        "            results['surprise_shifts'].append(0.0)\n",
        "\n",
        "        # Select from remaining\n",
        "        remaining = [i for i in range(N_POOL)\n",
        "                     if i not in all_selected and not is_dup[i]]\n",
        "        if len(remaining) < k:\n",
        "            new_selected = remaining\n",
        "        else:\n",
        "            Q, v2d = build_qubo(\n",
        "                current_surprises, signatures, simhashes, is_dup,\n",
        "                remaining[:min(len(remaining), 1000)],\n",
        "                K=k, alpha=1.0, beta=5.0, delta=0.3, gamma=10.0\n",
        "            )\n",
        "            sel_vars, _ = solve_qubo(Q, label=f'{run_name}-R{rd}')\n",
        "            new_selected = [v2d[v] for v in sel_vars if v in v2d]\n",
        "\n",
        "        all_selected.update(new_selected)\n",
        "        results['round_selections'].append(len(new_selected))\n",
        "\n",
        "        # Curriculum: sort ALL selected by current surprise (easy first)\n",
        "        sorted_sel = sorted(all_selected, key=lambda i: current_surprises[i])\n",
        "        curriculum_texts = [pool_texts[i] for i in sorted_sel]\n",
        "        curriculum_surprises = np.array([current_surprises[i] for i in sorted_sel])\n",
        "\n",
        "        # Weighted loss: softmax weights from surprise\n",
        "        exp_s = np.exp((curriculum_surprises - curriculum_surprises.max()) / tau)\n",
        "        weights = exp_s / exp_s.sum() * len(sorted_sel)\n",
        "\n",
        "        ds = WeightedTextDataset(curriculum_texts, weights, tokenizer)\n",
        "        # No shuffle — curriculum order preserved\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        sched = get_linear_schedule_with_warmup(opt, 10, len(loader))\n",
        "\n",
        "        model.train()\n",
        "        total, nb = 0, 0\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            w = batch[\"weight\"].to(device)\n",
        "\n",
        "            logits = model(input_ids=ids, attention_mask=mask).logits[:, :-1, :]\n",
        "            labels = ids[:, 1:]\n",
        "            m = mask[:, 1:]\n",
        "            pt = nn.CrossEntropyLoss(reduction='none')(\n",
        "                logits.reshape(-1, logits.size(-1)), labels.reshape(-1)\n",
        "            ).reshape(labels.shape)\n",
        "            doc_loss = (pt * m).sum(dim=1) / m.sum(dim=1).clamp(min=1)\n",
        "            weighted_loss = (doc_loss * w).mean()\n",
        "\n",
        "            weighted_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sched.step(); opt.zero_grad()\n",
        "            total += weighted_loss.item(); nb += 1\n",
        "\n",
        "        ppl = evaluate_ppl(model, test_texts, tokenizer)\n",
        "        results['train_losses'].append(total / nb)\n",
        "        results['eval_ppls'].append(ppl)\n",
        "\n",
        "        print(f\"  [{run_name}] Round {rd+1}: +{len(new_selected)} docs \"\n",
        "              f\"(total {len(all_selected)}), loss={total/nb:.4f}, ppl={ppl:.2f}\")\n",
        "\n",
        "    results['final_ppl'] = results['eval_ppls'][-1]\n",
        "    results['total_selected'] = len(all_selected)\n",
        "    del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\\nTraining: Full Pipeline (Active + Curriculum + Weighted)\")\n",
        "results_full = train_full_pipeline(\n",
        "    pool_texts, surprises, signatures, simhashes, is_duplicate,\n",
        "    test_texts, tokenizer, 'FullPipeline', tau=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-24",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: 結果比較"
      ]
    },
    {
      "id": "cell-25",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RESULTS: Smart Selection + Smart Processing\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_runs = [\n",
        "    ('Base (no FT)', base_ppl, '-', '-'),\n",
        "    ('Vanilla FT', results_vanilla['final_ppl'], 'Static QUBO', 'Vanilla'),\n",
        "    ('A: Curriculum', results_curriculum['final_ppl'], 'Static QUBO', 'Curriculum'),\n",
        "    ('B: Weighted (t=1.0)', results_weighted['final_ppl'], 'Static QUBO', 'Weighted'),\n",
        "    ('B: Weighted (t=0.5)', results_weighted_hot['final_ppl'], 'Static QUBO', 'Weighted-Hot'),\n",
        "    ('C: Active Iter', results_active['final_ppl'], 'Active QUBO', 'Vanilla'),\n",
        "    ('D: Full Pipeline', results_full['final_ppl'], 'Active QUBO', 'Curriculum+Weighted'),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Method':<25} {'PPL':>8} {'vs Vanilla':>12} {'Selection':>15} {'Processing':>20}\")\n",
        "print(\"-\" * 85)\n",
        "vanilla_ppl = results_vanilla['final_ppl']\n",
        "for name, ppl, sel, proc in all_runs:\n",
        "    if name == 'Base (no FT)':\n",
        "        vs = '---'\n",
        "    else:\n",
        "        delta = (ppl / vanilla_ppl - 1) * 100\n",
        "        vs = f\"{delta:+.2f}%\"\n",
        "    print(f\"{name:<25} {ppl:>8.2f} {vs:>12} {sel:>15} {proc:>20}\")\n",
        "\n",
        "# Find best\n",
        "best_name, best_ppl = min(all_runs[1:], key=lambda x: x[1])[:2]\n",
        "improvement = (1 - best_ppl / vanilla_ppl) * 100\n",
        "print(f\"\\nBest: {best_name} (PPL={best_ppl:.2f}, {improvement:+.2f}% vs vanilla)\")\n",
        "\n",
        "# Decompose improvement sources\n",
        "print(f\"\\n--- Improvement Decomposition ---\")\n",
        "smart_select_gain = (1 - vanilla_ppl / base_ppl) * 100\n",
        "curriculum_gain = (1 - results_curriculum['final_ppl'] / vanilla_ppl) * 100\n",
        "weighted_gain = (1 - results_weighted['final_ppl'] / vanilla_ppl) * 100\n",
        "active_gain = (1 - results_active['final_ppl'] / vanilla_ppl) * 100\n",
        "full_gain = (1 - results_full['final_ppl'] / vanilla_ppl) * 100\n",
        "\n",
        "print(f\"  Smart Selection alone (Exp2):  {smart_select_gain:+.2f}% PPL reduction from base\")\n",
        "print(f\"  + Curriculum:                  {curriculum_gain:+.2f}% additional from vanilla\")\n",
        "print(f\"  + Weighted Loss:               {weighted_gain:+.2f}% additional from vanilla\")\n",
        "print(f\"  + Active Iteration:            {active_gain:+.2f}% additional from vanilla\")\n",
        "print(f\"  + Full Pipeline (all three):   {full_gain:+.2f}% additional from vanilla\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-26",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: 可視化"
      ]
    },
    {
      "id": "cell-27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "epochs_range = list(range(1, TRAIN_EPOCHS + 1))\n",
        "\n",
        "# --- Plot 1: PPL learning curves ---\n",
        "ax = axes[0, 0]\n",
        "runs_to_plot = [\n",
        "    ('Vanilla', results_vanilla, 'gray', '--'),\n",
        "    ('Curriculum', results_curriculum, 'orange', '-'),\n",
        "    ('Weighted', results_weighted, 'purple', '-'),\n",
        "    ('Active', results_active, 'blue', '-'),\n",
        "    ('Full Pipeline', results_full, 'red', '-'),\n",
        "]\n",
        "for name, res, color, ls in runs_to_plot:\n",
        "    ax.plot(epochs_range, res['eval_ppls'], ls, color=color, linewidth=2,\n",
        "            marker='o', markersize=6, label=name)\n",
        "ax.axhline(base_ppl, color='lightgray', linestyle=':', label=f'Base: {base_ppl:.1f}')\n",
        "ax.set_xlabel('Epoch / Round')\n",
        "ax.set_ylabel('Perplexity')\n",
        "ax.set_title('PPL Learning Curves')\n",
        "ax.legend(fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 2: Final PPL bar chart ---\n",
        "ax = axes[0, 1]\n",
        "names = ['Base', 'Vanilla', 'Curric.', 'Weight.', 'Active', 'Full']\n",
        "ppls = [base_ppl, vanilla_ppl, results_curriculum['final_ppl'],\n",
        "        results_weighted['final_ppl'], results_active['final_ppl'],\n",
        "        results_full['final_ppl']]\n",
        "colors = ['lightgray', 'gray', 'orange', 'purple', 'blue', 'red']\n",
        "bars = ax.bar(names, ppls, color=colors, alpha=0.8, edgecolor='black')\n",
        "for bar, ppl in zip(bars, ppls):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
        "            f'{ppl:.1f}', ha='center', fontsize=9, fontweight='bold')\n",
        "ax.set_ylabel('Perplexity (lower = better)')\n",
        "ax.set_title('Final PPL Comparison')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# --- Plot 3: Improvement decomposition ---\n",
        "ax = axes[0, 2]\n",
        "components = ['Selection\\n(Exp 0-2)', 'Curriculum\\n(Strat A)', 'Weighted\\n(Strat B)',\n",
        "              'Active\\n(Strat C)', 'Full\\n(A+B+C)']\n",
        "gains = [smart_select_gain, curriculum_gain, weighted_gain, active_gain, full_gain]\n",
        "bar_colors = ['gray', 'orange', 'purple', 'blue', 'red']\n",
        "bars = ax.bar(components, gains, color=bar_colors, alpha=0.8, edgecolor='black')\n",
        "for bar, g in zip(bars, gains):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
        "            f'{g:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
        "ax.set_ylabel('PPL Reduction (%)')\n",
        "ax.set_title('Improvement Sources')\n",
        "ax.axhline(0, color='black', linewidth=0.5)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# --- Plot 4: Active iteration surprise shift ---\n",
        "ax = axes[1, 0]\n",
        "if results_active['surprise_shifts']:\n",
        "    rounds_x = list(range(1, len(results_active['surprise_shifts']) + 1))\n",
        "    ax.bar(rounds_x, results_active['surprise_shifts'], color='blue', alpha=0.7)\n",
        "    ax.set_xlabel('Round')\n",
        "    ax.set_ylabel('Mean Surprise Shift')\n",
        "    ax.set_title('Active Iteration: Surprise Distribution Shift')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 5: Weighted loss weight distribution ---\n",
        "ax = axes[1, 1]\n",
        "s_scores = np.array([surprises[i] for i in static_selected])\n",
        "for tau, color, label in [(0.5, 'red', 'tau=0.5'), (1.0, 'blue', 'tau=1.0'),\n",
        "                           (2.0, 'green', 'tau=2.0')]:\n",
        "    exp_s = np.exp((s_scores - s_scores.max()) / tau)\n",
        "    w = exp_s / exp_s.sum() * len(s_scores)\n",
        "    ax.scatter(s_scores, w, alpha=0.3, s=20, color=color, label=label)\n",
        "ax.set_xlabel('Surprise')\n",
        "ax.set_ylabel('Loss Weight')\n",
        "ax.set_title('Surprise-Weighted Loss: Weight Distribution')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 6: Train loss curves ---\n",
        "ax = axes[1, 2]\n",
        "for name, res, color, ls in runs_to_plot:\n",
        "    ax.plot(epochs_range, res['train_losses'], ls, color=color, linewidth=2,\n",
        "            marker='s', markersize=6, label=name)\n",
        "ax.set_xlabel('Epoch / Round')\n",
        "ax.set_ylabel('Train Loss')\n",
        "ax.set_title('Training Loss Curves')\n",
        "ax.legend(fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiment3_results.png', dpi=150, bbox_inches='tight')\n",
        "print(\"Saved: experiment3_results.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cell-28",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: まとめ"
      ]
    },
    {
      "id": "cell-29",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EXPERIMENT 3 COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "Question: 賢く選んで、賢く処理する — できてる？\n",
        "\n",
        "Answer:\n",
        "\n",
        "  賢く選ぶ (Smart Selection):\n",
        "    - Surprise scoring (proxy model inference)\n",
        "    - MinHash LSH deduplication\n",
        "    - SimHash diversity fingerprinting\n",
        "    - Hierarchical QUBO optimization\n",
        "    → PPL reduction from base: {smart_select_gain:+.2f}%\n",
        "\n",
        "  賢く処理する (Smart Processing):\n",
        "    A. Curriculum Learning (easy -> hard)    → {curriculum_gain:+.2f}% vs vanilla\n",
        "    B. Surprise-Weighted Loss                → {weighted_gain:+.2f}% vs vanilla\n",
        "    C. Active Iteration (dynamic re-scoring) → {active_gain:+.2f}% vs vanilla\n",
        "    D. Full Pipeline (A + B + C)             → {full_gain:+.2f}% vs vanilla\n",
        "\n",
        "  Total improvement (best):  Base PPL {base_ppl:.2f} → {best_ppl:.2f}\n",
        "\n",
        "Experiment Series Complete:\n",
        "  Exp 0: Can QUBO select high-surprise data?        → Yes\n",
        "  Exp 1: Does it scale to trillion tokens?           → Architecture validated\n",
        "  Exp 2: Does it improve downstream training?        → PPL improvement confirmed\n",
        "  Exp 3: Does smart processing add to smart selection? → {'+' if full_gain < 0 else ''}Combined pipeline is strongest\n",
        "\"\"\")\n",
        "\n",
        "# Save results\n",
        "results_json = {\n",
        "    'base_ppl': float(base_ppl),\n",
        "    'vanilla': {'ppl': float(vanilla_ppl), 'curve': results_vanilla['eval_ppls']},\n",
        "    'curriculum': {'ppl': float(results_curriculum['final_ppl']),\n",
        "                   'curve': results_curriculum['eval_ppls']},\n",
        "    'weighted_1.0': {'ppl': float(results_weighted['final_ppl']),\n",
        "                     'curve': results_weighted['eval_ppls']},\n",
        "    'weighted_0.5': {'ppl': float(results_weighted_hot['final_ppl']),\n",
        "                     'curve': results_weighted_hot['eval_ppls']},\n",
        "    'active': {'ppl': float(results_active['final_ppl']),\n",
        "               'curve': results_active['eval_ppls'],\n",
        "               'surprise_shifts': results_active['surprise_shifts']},\n",
        "    'full_pipeline': {'ppl': float(results_full['final_ppl']),\n",
        "                      'curve': results_full['eval_ppls'],\n",
        "                      'surprise_shifts': results_full['surprise_shifts']},\n",
        "    'gains': {\n",
        "        'selection': float(smart_select_gain),\n",
        "        'curriculum': float(curriculum_gain),\n",
        "        'weighted': float(weighted_gain),\n",
        "        'active': float(active_gain),\n",
        "        'full': float(full_gain),\n",
        "    }\n",
        "}\n",
        "with open('experiment3_results.json', 'w') as f:\n",
        "    json.dump(results_json, f, indent=2)\n",
        "print(\"Results saved: experiment3_results.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
